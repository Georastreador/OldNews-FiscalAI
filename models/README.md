# ü§ñ Modelos de IA - OldNews FiscalAI

Este diret√≥rio cont√©m os modelos de intelig√™ncia artificial utilizados pelo sistema.

## üìã Modelos Suportados

### üè† Modelos Locais

#### Mistral 7B Instruct
- **Arquivo**: `mistral-7b-instruct-v0.1.Q4_K_M.gguf`
- **Tamanho**: ~4.1GB
- **Formato**: GGUF (Quantizado)
- **Uso**: An√°lise de texto e classifica√ß√£o NCM

**Download:**
```bash
# Baixar modelo (n√£o inclu√≠do no reposit√≥rio por tamanho)
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

### ‚òÅÔ∏è Modelos em Nuvem

#### OpenAI
- **GPT-3.5-turbo**: An√°lise r√°pida e econ√¥mica
- **GPT-4**: An√°lise avan√ßada e precisa
- **Configura√ß√£o**: Via vari√°vel `OPENAI_API_KEY`

#### Anthropic
- **Claude-3-sonnet**: An√°lise equilibrada
- **Claude-3-opus**: An√°lise mais avan√ßada
- **Configura√ß√£o**: Via vari√°vel `ANTHROPIC_API_KEY`

#### Google
- **Gemini-pro**: An√°lise multimodal
- **Configura√ß√£o**: Via vari√°vel `GOOGLE_API_KEY`

#### Groq
- **Llama-2-70b**: An√°lise r√°pida
- **Mixtral-8x7b**: An√°lise eficiente
- **Configura√ß√£o**: Via vari√°vel `GROQ_API_KEY`

## ‚öôÔ∏è Configura√ß√£o

### 1. Modelo Local

```python
from src.utils.model_manager import get_model_manager

# Configurar modelo local
model_manager = get_model_manager()
llm = model_manager.get_llm("mistral-7b-local")
```

### 2. Modelo em Nuvem

```python
# Configurar OpenAI
llm = model_manager.get_llm("gpt-3.5-turbo")

# Configurar Anthropic
llm = model_manager.get_llm("claude-3-sonnet")

# Configurar Google
llm = model_manager.get_llm("gemini-pro")

# Configurar Groq
llm = model_manager.get_llm("llama-2-70b")
```

## üìä Compara√ß√£o de Modelos

| Modelo | Velocidade | Precis√£o | Custo | Privacidade |
|--------|------------|----------|-------|-------------|
| Mistral 7B Local | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| GPT-3.5-turbo | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| GPT-4 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Claude-3-sonnet | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Gemini-pro | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Llama-2-70b (Groq) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

## üîß Requisitos de Sistema

### Modelo Local (Mistral 7B)
- **RAM**: M√≠nimo 8GB, recomendado 16GB
- **Armazenamento**: 5GB livres
- **CPU**: Qualquer processador moderno
- **GPU**: Opcional, acelera infer√™ncia

### Modelos em Nuvem
- **Internet**: Conex√£o est√°vel
- **API Keys**: Configuradas corretamente
- **Rate Limits**: Respeitar limites da API

## üöÄ Performance

### Tempos de An√°lise (NF-e t√≠pica)
- **Mistral 7B Local**: 30-60 segundos
- **GPT-3.5-turbo**: 10-20 segundos
- **GPT-4**: 20-40 segundos
- **Claude-3-sonnet**: 15-30 segundos
- **Gemini-pro**: 10-25 segundos
- **Llama-2-70b (Groq)**: 5-15 segundos

## üìù Notas Importantes

1. **Modelos Locais**: Oferecem m√°xima privacidade mas requerem mais recursos
2. **Modelos em Nuvem**: Mais r√°pidos mas enviam dados para terceiros
3. **Custos**: Modelos em nuvem t√™m custos por token
4. **Rate Limits**: APIs t√™m limites de requisi√ß√µes por minuto
5. **Backup**: Sempre mantenha backup dos modelos locais

## üîÑ Atualiza√ß√µes

Para atualizar modelos:

```bash
# Atualizar depend√™ncias
pip install --upgrade -r requirements.txt

# Baixar novo modelo local
wget [URL_DO_NOVO_MODELO]

# Testar novo modelo
python -c "from src.utils.model_manager import get_model_manager; print('‚úÖ OK')"
```

---

**Para mais informa√ß√µes, consulte a [documenta√ß√£o completa](../docs/configuration.md)**
