#!/usr/bin/env python3
"""
Teste das corre√ß√µes no chat do modelo local
"""

import sys
from pathlib import Path
import os
from dotenv import load_dotenv

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.utils.model_manager import get_model_manager
from src.models import LLMConfig, LLMProvider

load_dotenv()

def test_model_config():
    """Testa se a configura√ß√£o do modelo est√° correta"""
    print("üîß Testando configura√ß√£o do modelo...")
    
    manager = get_model_manager()
    
    # Testar configura√ß√£o padr√£o
    config_default = manager._load_default_config()
    print(f"‚úÖ Configura√ß√£o padr√£o:")
    print(f"  Provider: {config_default.provider}")
    print(f"  Model: {config_default.model}")
    print(f"  Temperature: {config_default.temperature}")
    print(f"  Max tokens: {config_default.max_tokens}")
    
    # Testar configura√ß√£o de modelo espec√≠fico
    llm = manager.get_llm("mistral-7b-gguf")
    print(f"‚úÖ LLM obtido: {type(llm).__name__}")
    
    return True

def test_response_cleaning():
    """Testa a limpeza de respostas"""
    print("\nüßπ Testando limpeza de respostas...")
    
    from src.utils.model_manager import LocalGGUFWrapper
    from src.models import LLMConfig, LLMProvider
    
    # Criar wrapper de teste
    config = LLMConfig(
        provider=LLMProvider.LOCAL,
        model="test",
        temperature=0.1,
        max_tokens=512
    )
    
    # Mock do local_manager
    class MockLocalManager:
        def generate_response(self, model_name, prompt, **kwargs):
            return "ASSISTANT: Esta √© uma resposta de teste. Esta √© uma resposta de teste."
    
    wrapper = LocalGGUFWrapper(MockLocalManager(), "test", config)
    
    # Testar limpeza
    test_response = "ASSISTANT: Esta √© uma resposta de teste. Esta √© uma resposta de teste."
    cleaned = wrapper._clean_response(test_response)
    
    print(f"‚úÖ Resposta original: {test_response}")
    print(f"‚úÖ Resposta limpa: {cleaned}")
    
    return "ASSISTANT:" not in cleaned and len(cleaned.split('\n')) <= 2

def test_simple_prompt():
    """Testa um prompt simples com o modelo local"""
    print("\nüí¨ Testando prompt simples...")
    
    try:
        manager = get_model_manager()
        llm = manager.get_llm("mistral-7b-gguf")
        
        prompt = "Responda em uma frase: Qual √© a capital do Brasil?"
        response = llm.invoke(prompt, max_tokens=50)
        
        print(f"‚úÖ Prompt: {prompt}")
        print(f"‚úÖ Resposta: {response}")
        
        # Verificar se a resposta √© razo√°vel
        return len(response) < 200 and "Bras√≠lia" in response or "bras√≠lia" in response
        
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        return False

def test_fiscal_prompt():
    """Testa um prompt fiscal espec√≠fico"""
    print("\nüìä Testando prompt fiscal...")
    
    try:
        manager = get_model_manager()
        llm = manager.get_llm("mistral-7b-gguf")
        
        prompt = """Voc√™ √© um assistente fiscal especializado. Responda de forma clara e concisa.

CONTEXTO: NF-e R$ 1.000, 5 itens, risco 25/100, 0 fraudes

PERGUNTA: Qual o valor total das NFs?

INSTRU√á√ïES:
1. Responda APENAS √† pergunta feita
2. Seja direto e objetivo (m√°ximo 3 par√°grafos)
3. Use dados espec√≠ficos do relat√≥rio quando dispon√≠vel
4. Se n√£o souber, diga "N√£o tenho essa informa√ß√£o no relat√≥rio"
5. Use **negrito** para destacar informa√ß√µes importantes
6. Termine a resposta com um ponto final

RESPOSTA:"""
        
        response = llm.invoke(prompt, max_tokens=100)
        
        print(f"‚úÖ Prompt fiscal testado")
        print(f"‚úÖ Resposta: {response}")
        
        # Verificar se a resposta √© relevante
        return len(response) < 300 and ("R$" in response or "valor" in response.lower())
        
    except Exception as e:
        print(f"‚ùå Erro no teste fiscal: {e}")
        return False

def main():
    """Fun√ß√£o principal de teste"""
    print("üß™ TESTE DAS CORRE√á√ïES NO CHAT DO MODELO LOCAL")
    print("=" * 60)
    
    results = {}
    
    # Teste 1: Configura√ß√£o
    print("\nüî¨ Teste 1: Configura√ß√£o do Modelo")
    print("-" * 40)
    results["config"] = test_model_config()
    print(f"‚úÖ Configura√ß√£o: {'PASSOU' if results['config'] else 'FALHOU'}")
    
    # Teste 2: Limpeza de respostas
    print("\nüî¨ Teste 2: Limpeza de Respostas")
    print("-" * 40)
    results["cleaning"] = test_response_cleaning()
    print(f"‚úÖ Limpeza: {'PASSOU' if results['cleaning'] else 'FALHOU'}")
    
    # Teste 3: Prompt simples
    print("\nüî¨ Teste 3: Prompt Simples")
    print("-" * 40)
    results["simple"] = test_simple_prompt()
    print(f"‚úÖ Prompt Simples: {'PASSOU' if results['simple'] else 'FALHOU'}")
    
    # Teste 4: Prompt fiscal
    print("\nüî¨ Teste 4: Prompt Fiscal")
    print("-" * 40)
    results["fiscal"] = test_fiscal_prompt()
    print(f"‚úÖ Prompt Fiscal: {'PASSOU' if results['fiscal'] else 'FALHOU'}")
    
    print("\n" + "=" * 60)
    print("üìä RESUMO DOS TESTES")
    print("=" * 60)
    
    for test_name, passed in results.items():
        print(f"{test_name.replace('_', ' ').title()}: {'‚úÖ PASSOU' if passed else '‚ùå FALHOU'}")
    
    all_passed = all(results.values())
    print(f"\nüéØ Resultado Final: {sum(results.values())}/{len(results)} testes passaram")
    
    if all_passed:
        print("üéâ Todas as corre√ß√µes funcionaram! O chat deve estar melhor agora.")
    else:
        print("‚ö†Ô∏è Alguns testes falharam. Verifique os logs acima.")
    
    return all_passed

if __name__ == "__main__":
    main()
